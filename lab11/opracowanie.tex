\documentclass{article}

\usepackage{graphicx}
\usepackage{systeme}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{amsmath}



\title{Lab 11}
\author{Filip Jędrzejewski}

\begin{document}
	\maketitle

	
	\section*{Zadanie 1}


	\subsection*{Opis problemu}

	Celem zadania było wyznaczenie punktów krytycznych każdej z poniższych funkcji. Następnie należało scharatkeryzować każdy znaleziony punkt jako minimum, maksimum lub punkt siodłowy. Dla każdej funkcji zbadano również, czy posiada minimum lub maksimum globalne na zbiorze $\mathbb{R}^2$.

	\subsection*{$f_1(x,y) = x^2 - 4xy + y^2$}

	Gradient oraz Hessian:

	\begin{equation}
		\nabla f_1(x,y) = \begin{bmatrix} 2x-4y \\ 2y-4x \end{bmatrix}
	\end{equation}

	\begin{equation}
		H_1(x,y) = \begin{bmatrix} 2 & -4\\ -4 & 2\end{bmatrix}
	\end{equation}

	\begin{equation}
		D_1(x,y) = det(H_1(x,y)) = -12
	\end{equation}

	Wyznaczanie punktów krytycznych:

	\begin{equation}
		\systeme{2x-4y = 0,2y - 4x = 0}
	\end{equation}

	\begin{equation}
		(x,y) = (0,0)
	\end{equation}

	Wyznacznik hessianu dla $(0,0)$ jest ujemny, zatem jest to punkt siodłowy. Funkcja $f_1(x,y)$ nie ma innych punktów krytycznych, zatem nie ma maksimum lub minimum globalnego.


	\subsection*{$f_2(x,y) = x^4 - 4xy + y^4$}

	Gradient oraz Hessian:

	\begin{equation}
		\nabla f_2(x,y) = \begin{bmatrix} 4x^3-4y \\ 4y^3-4x \end{bmatrix}
	\end{equation}

	\begin{equation}
		H_2(x,y) = \begin{bmatrix} 12x^2 & -4\\ -4 & 12y^2\end{bmatrix}
	\end{equation}

	\begin{equation}
		D_2(x,y) = det(H_2(x,y)) = 144x^2y^2-16
	\end{equation}

	Wyznaczanie punktów krytycznych:

	\begin{equation}
		\systeme{4x^3-4y = 0,4y^3-4x = 0}
	\end{equation}

	\begin{equation}
		(x_1,y_1) = (0,0)
	\end{equation}

	\begin{equation}
		(x_2,y_2) = (1,1)
	\end{equation}

	\begin{equation}
		(x_3,y_3) = (-1,-1)
	\end{equation}

	Wyznacznik hessianu dla $(0,0)$ jest ujemny, zatem jest to punkt siodłowy. Hessian dla punktów $(1,1)$ i $(-1,-1)$ ma wyznacznik dodatni, oraz odpowiednie drugie pochodne są dodatnie, z tego wynika, że są to minima.
	
	\begin{equation}
		f_2(-1,-1) = f_2(1,1) = -2
	\end{equation}

	Są to minima globalne.


	\subsection*{$f_3(x,y) = 2x^3 - 3x^2 - 6xy(x-y-1)$}

	Po przekształceniu:

	\begin{equation}
		f_3(x,y) = 2x^3-3x^2-6x^2y+6xy^2+6xy
	\end{equation}

	Gradient oraz Hessian:

	\begin{equation}
		\nabla f_3(x,y) = \begin{bmatrix} 6x^2-6x-12xy+6y^2+6y \\ 12xy+6x-6x^2 \end{bmatrix}
	\end{equation}

	\begin{equation}
		H_3(x,y) = \begin{bmatrix} 12x-6-12y & 12y+6-12x\\ 12y+6-12x & 12x\end{bmatrix}
	\end{equation}

	\begin{equation}
		D_3(x,y) = det(H_3(x,y)) = 36 \cdot (4xy+2x-4y-4y^2-1)
	\end{equation}

	\newpage

	Wyznaczanie punktów krytycznych:

	\begin{equation}
		\nabla f_3(x,y) = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
	\end{equation}

	\begin{equation}
		(x_1,y_1) = (0,0)
	\end{equation}

	\begin{equation}
		(x_2,y_2) = (1,0)
	\end{equation}

	\begin{equation}
		(x_3,y_3) = (0,-1)
	\end{equation}

	\begin{equation}
		(x_4,y_4) = (-1,-1)
	\end{equation}

	Wyznacznik hessianu dla $(0,0)$ oraz $(0, -1)$ jest ujemny, zatem jest to punkt siodłowy. Dla $(1,0)$ wyznacznik hessianu jest dodatni i odpowiednie drugie pochodne są dodatnie, zatem jest to minimum lokalne. Dla punktu $(-1,-1)$ wyznacznik hessianu również jest dodatni, jednak odpowiednie drugie pochodne są ujemne, zatem jest to maksimum lokalne. Funkcja $f_3(x,y)$ nie ma maksimum lub minimum globalnego.


	\subsection*{$f_4(x,y) = (x-y)^4 + x^2 - y^2 - 2x + 2y + 1$}

	Gradient oraz Hessian:

	\begin{equation}
		\nabla f_4(x,y) = \begin{bmatrix} 4(x-y)^3+2x-2 \\ -4(x-y)^3+-2y+2 \end{bmatrix}
	\end{equation}

	\begin{equation}
		H_4(x,y) = \begin{bmatrix} 12(x-y)^2+2 & -12(x-y)^2\\ -12(x-y)^2 & 12(x-y)^2-2\end{bmatrix}
	\end{equation}

	\begin{equation}
		D_4(x,y) = det(H_4(x,y)) = -4
	\end{equation}

	Wyznaczanie punktów krytycznych:

	\begin{equation}
		\nabla f_4(x,y) = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
	\end{equation}

	Rozwiązanie:
	
	\begin{equation}
		(x,y) = (1,1)
	\end{equation}

	Wyznacznik hessianu dla tego punktu jest ujemny, zatem jest to punkt siodłowy. Funkcja $f_4(x,y)$ nie ma maksimum lub minimum globalnego.

	\newpage

	\section*{Zadanie 2}


	\subsection*{Opis problemu}

	Celem zadania było napisanie programu znajdującego minimum funkcji Rosenbrocka:

	\begin{equation}
		f(x,y) = 100(y-x^2)+(1-x)^2
	\end{equation}

	\subsection*{Metoda największego spadku}

	W celu znalezienia minimum danej funkcji metodą największego spadku, zaimplementowano następującą funkcję:

	\begin{lstlisting}[language=Python]
def steepest_descent(x0, N):

  #funkcja pomocnicza minimalizujaca alphe za pomoca metody zlotego podzialu
  def get_alpha(x, g):
    a = 0
    b = 1
    phi = (np.sqrt(5) - 1) / 2
    c = a + (b - a) * phi
    d = b - (b - a) * phi
    while abs(b - a) > 0.001:
        if f(x-d*g) < f(x-c*g):
            b = c
        else:
            a = d
        c = a + (b - a) * phi
        d = b - (b - a) * phi
    return (a + b) / 2

  #kopia dla niezmiennosci
  x = x0.copy()

  #glowna petla
  for i in range(N):
    #wyznaczam gradient
    g = gradient(x)

    #minimalizuje wspolczynnik alpha
    alpha = get_alpha(x, g)
    #wyznaczam nowego x
    x -= alpha * g
    
  #return wyniku
  return x
	\end{lstlisting}

	Funkcja przyjmuje jako parametry punkt początkowy $x_0$ oraz oczekiwaną liczbę iteracji $N$. Funkcja korzysta ze zdefiniowanej globalnie metody \texttt{gradient(x)}, która wyznacza gradient funkcji \texttt{f} w oczekiwanym punkcie \texttt{x}.


	\subsection*{Metoda Newton'a}

	W celu znalezienia minimum danej funkcji metodą Newton'a, zaimplementowano następującą funkcję:

	\begin{lstlisting}[language=Python]
def newton(x0, N):
   #kopia dla bezpieczenstwa
   x = x0.copy()
		
   #glowna petla
   for i in range(N):
     #obliczam gradient i hessian
     g = gradient(x)
     h = hessian(x)
	  
     #odwracam hessian, jesli jest osobliwy, 
     #to go lekko zmieniam, zeby byl odwracalny
     if np.linalg.det(h) == 0:
   	h += np.eye(2) * 0.0000001
     h_inv = np.linalg.inv(h)
	  
     #wyznaczam kolejnego x
     x -= h_inv.dot(g)
		
   #return wyniku
   return x
	\end{lstlisting}

	Funkcja przyjmuje identyczne parametry jak \texttt{steepest descent}, jednak wykorzystuje ona również zdefiniowaną globalnie metodę \texttt{hessian(x)}, która wyznacza hessian funkcji \texttt{f} w oczekiwanym punkcie \texttt{x}.

	\subsection*{Wyniki i wnioski}

	Program wykorzystywał globlanie zdefiniowane funkcje obliczające wartość badanej funkcji, jej gradient oraz hessian dla danego punktu $x$:

	\begin{lstlisting}[language=Python]
#funkcja Rosenbrocka
f = lambda x: 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2

#gradient i hessian
gradient = lambda x: np.array([-400*x[0]*x[1] + 400*x[0]**3 - 4 + 2*x[0],
				200*x[1] - 200*x[0]**2])
hessian = lambda x: np.array([[-400*x[1] + 1200*x[0]**2, -400*x[0]], 
				[-400*x[0], 200]])

	\end{lstlisting}

	Zdefiniowane funkcje przetestowano z następującymi punktami startowymi z liczbą iteracji równą $10$ ($N=10$):

	\begin{equation}
		x_1 = \begin{bmatrix} -1 \\ 1 \end{bmatrix}
	\end{equation}
	\begin{equation}
		x_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}
	\end{equation}
	\begin{equation}
		x_3 = \begin{bmatrix} 2 \\ 1 \end{bmatrix}
	\end{equation}

	Kod wywołujący funkcje:

	\begin{lstlisting}[language=Python]
#wywolania
X = [
		np.array([-1., 1.]),
		np.array([0., 1.]),
		np.array([2., 1.])
	]
		
for x in X:
	print("--------------------------------------------------")
	print("x_0 = (", x[0], ",", x[1], ")")
	print("Metoda najwiekszego spadku:", steepest_descent(x, 10))
	print("Metoda Newtona:", newton(x, 10))
		
	\end{lstlisting}


	Otrzymane wyniki:

	\begin{lstlisting}[language=bash]
----------------------------------------------------
x_0 = ( -1.0 , 1.0 )
Metoda najwiekszego spadku: [1.00279902 1.00169753]
Metoda Newtona: [-1.36530161e+08  1.86404850e+16]
----------------------------------------------------
x_0 = ( 0.0 , 1.0 )
Metoda najwiekszego spadku: [1.16503372 1.35430657]
Metoda Newtona: [-2.33524178e+08  5.18416905e+16]
----------------------------------------------------
x_0 = ( 2.0 , 1.0 )
Metoda najwiekszego spadku: [1.70463885 2.90499583]
Metoda Newtona: [-4.09068824e+09  1.67337303e+19]
				
	\end{lstlisting}

	Oczekiwanym wynikiem (czyli minimum globalnym funkcji Rosenbrocka) był punkt:

	\begin{equation}
		x_0 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}
	\end{equation}

	Najlepsze wyniki dawała metoda najszybszego spadku.

	\newpage

	Po zauważeniu dużych niezgodności pomiędzy otrzymywanymi wynikami, a tymi oczekiwanymi, przetestowano również zaimplementowane funkcje dla:

	\begin{equation}
		f(x,y) = x^2+y^2+5x
	\end{equation}

	Z oczekiwanym minimum:

	\begin{equation}
		x_0 = \begin{bmatrix} -\frac{5}{2} \\ \frac{3}{2} \end{bmatrix}  = \begin{bmatrix} -2.5 \\ 1.5 \end{bmatrix}
	\end{equation}

	Kod funkcji, jej gradientu i hessianu:

	\begin{lstlisting}[language=Python]
#funkcja testowa
f = lambda x: x[0]**2 + x[1]**2 + 5*x[0] - 3*x[1]    #minimum -> (-2.5, 1.5)
gradient = lambda x: np.array([2*x[0]+5, 2*x[1]-3])
hessian = lambda x: np.array([[2, 0], [0, 2]])
	\end{lstlisting}

	Otrzymane wyniki:

	\begin{lstlisting}[language=bash]
----------------------------------------------------
x_0 = ( -1.0 , 1.0 )
Metoda najwiekszego spadku: [-2.5  1.5]
Metoda Newtona: [-2.5  1.5]
----------------------------------------------------
x_0 = ( 0.0 , 1.0 )
Metoda najwiekszego spadku: [-2.5  1.5]
Metoda Newtona: [-2.5  1.5]
----------------------------------------------------
x_0 = ( 2.0 , 1.0 )
Metoda najwiekszego spadku: [-2.5  1.5]
Metoda Newtona: [-2.5  1.5]
						
	\end{lstlisting}

	Wyniki w tym przypadku były poprawne.





	
	
\end{document}